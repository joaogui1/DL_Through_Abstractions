{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# Derivative of tanh from its output\n",
    "def dsig(y):\n",
    "    return y*(1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The neural network framework\n",
    "class Layer:\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "    # Init all weights between [-1 .. 1].\n",
    "    # Each input is connected to all outputs.\n",
    "    # One line per input and one column per output.\n",
    "        self.weights = np.random.uniform(-1, 1, (num_inputs, num_outputs))\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.output = sigmoid(input.dot(self.weights))\n",
    "        return self.output\n",
    "\n",
    "    def computeGradient(self, error):\n",
    "        self.delta = error * dsig(self.output)\n",
    "        # Returns the gradient\n",
    "        return self.delta.dot(self.weights.T)\n",
    "\n",
    "    def updateWeights(self, inputs, learning_rate):\n",
    "        self.weights += inputs.T.dot(self.delta) * learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "\n",
    "    def backprop(self, inp, learning_rate, error):\n",
    "        # Compute deltas at each layer starting from the last one\n",
    "        for layer in reversed(self.layers):\n",
    "            error = layer.computeGradient(error)\n",
    "\n",
    "    # Update the weights\n",
    "        for layer in self.layers:\n",
    "            layer.updateWeights(inp, learning_rate)\n",
    "            inp = layer.output\n",
    "            \n",
    "    def train(self, inputs, targets, epochs=50, learning_rate=0.01):\n",
    "        errors = []\n",
    "        for _ in range(epochs):\n",
    "            for inp, target in zip(inputs, targets):\n",
    "                output = self.forward(inp)\n",
    "                error = (target - np.argmax(output))\n",
    "                errors.append(error)\n",
    "                self.backprop(inp, learning_rate, error)\n",
    "            mse = (np.array(errors) ** 2).mean()\n",
    "            print(mse)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<178x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 178 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "wine.target = wine.target.reshape(-1, 1)\n",
    "wine.target = enc.fit_transform(wine.target)\n",
    "wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.target = np.array(wine.target.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(142, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = nn.Network([nn.Layer(13, 40),\n",
    "                 nn.Layer(40, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   0   MSE: 0.660\n",
      "-141\n",
      "  Epoch  10   MSE: 0.652\n",
      "-138\n",
      "  Epoch  20   MSE: 0.653\n",
      "-141\n",
      "  Epoch  30   MSE: 0.640\n",
      "-138\n",
      "  Epoch  40   MSE: 0.636\n",
      "-137\n",
      "  Epoch  50   MSE: 0.641\n",
      "-139\n",
      "  Epoch  60   MSE: 0.663\n",
      "-141\n",
      "  Epoch  70   MSE: 0.671\n",
      "-141\n",
      "  Epoch  80   MSE: 0.665\n",
      "-141\n",
      "  Epoch  90   MSE: 0.650\n",
      "-141\n",
      "  Epoch 100   MSE: 0.646\n",
      "-140\n",
      "  Epoch 110   MSE: 0.646\n",
      "-140\n",
      "  Epoch 120   MSE: 0.636\n",
      "-141\n",
      "  Epoch 130   MSE: 0.655\n",
      "-141\n",
      "  Epoch 140   MSE: 0.665\n",
      "-141\n",
      "  Epoch 150   MSE: 0.649\n",
      "-141\n",
      "  Epoch 160   MSE: 0.650\n",
      "-141\n",
      "  Epoch 170   MSE: 0.636\n",
      "-141\n",
      "  Epoch 180   MSE: 0.659\n",
      "-141\n",
      "  Epoch 190   MSE: 0.653\n",
      "-138\n",
      "  Epoch 200   MSE: 0.678\n",
      "-141\n",
      "  Epoch 210   MSE: 0.665\n",
      "-141\n",
      "  Epoch 220   MSE: 0.653\n",
      "-141\n",
      "  Epoch 230   MSE: 0.670\n",
      "-141\n",
      "  Epoch 240   MSE: 0.641\n",
      "-136\n",
      "  Epoch 250   MSE: 0.660\n",
      "-141\n",
      "  Epoch 260   MSE: 0.663\n",
      "-140\n",
      "  Epoch 270   MSE: 0.644\n",
      "-140\n",
      "  Epoch 280   MSE: 0.642\n",
      "-140\n",
      "  Epoch 290   MSE: 0.665\n",
      "-141\n",
      "  Epoch 300   MSE: 0.644\n",
      "-141\n",
      "  Epoch 310   MSE: 0.664\n",
      "-141\n",
      "  Epoch 320   MSE: 0.666\n",
      "-141\n",
      "  Epoch 330   MSE: 0.666\n",
      "-141\n",
      "  Epoch 340   MSE: 0.657\n",
      "-141\n",
      "  Epoch 350   MSE: 0.654\n",
      "-141\n",
      "  Epoch 360   MSE: 0.680\n",
      "-140\n",
      "  Epoch 370   MSE: 0.625\n",
      "-140\n",
      "  Epoch 380   MSE: 0.668\n",
      "-141\n",
      "  Epoch 390   MSE: 0.638\n",
      "-141\n",
      "  Epoch 400   MSE: 0.645\n",
      "-139\n",
      "  Epoch 410   MSE: 0.665\n",
      "-141\n",
      "  Epoch 420   MSE: 0.647\n",
      "-141\n",
      "  Epoch 430   MSE: 0.662\n",
      "-141\n",
      "  Epoch 440   MSE: 0.653\n",
      "-140\n",
      "  Epoch 450   MSE: 0.681\n",
      "-140\n",
      "  Epoch 460   MSE: 0.645\n",
      "-141\n",
      "  Epoch 470   MSE: 0.666\n",
      "-141\n",
      "  Epoch 480   MSE: 0.669\n",
      "-141\n",
      "  Epoch 490   MSE: 0.636\n",
      "-136\n",
      "  Epoch 500   MSE: 0.653\n",
      "-140\n",
      "  Epoch 510   MSE: 0.662\n",
      "-141\n",
      "  Epoch 520   MSE: 0.664\n",
      "-140\n",
      "  Epoch 530   MSE: 0.654\n",
      "-139\n",
      "  Epoch 540   MSE: 0.633\n",
      "-139\n",
      "  Epoch 550   MSE: 0.665\n",
      "-141\n",
      "  Epoch 560   MSE: 0.669\n",
      "-141\n",
      "  Epoch 570   MSE: 0.660\n",
      "-141\n",
      "  Epoch 580   MSE: 0.662\n",
      "-141\n",
      "  Epoch 590   MSE: 0.632\n",
      "-141\n",
      "  Epoch 600   MSE: 0.669\n",
      "-141\n",
      "  Epoch 610   MSE: 0.671\n",
      "-141\n",
      "  Epoch 620   MSE: 0.665\n",
      "-141\n",
      "  Epoch 630   MSE: 0.663\n",
      "-141\n",
      "  Epoch 640   MSE: 0.661\n",
      "-140\n",
      "  Epoch 650   MSE: 0.630\n",
      "-141\n",
      "  Epoch 660   MSE: 0.619\n",
      "-133\n",
      "  Epoch 670   MSE: 0.661\n",
      "-141\n",
      "  Epoch 680   MSE: 0.651\n",
      "-141\n",
      "  Epoch 690   MSE: 0.662\n",
      "-136\n",
      "  Epoch 700   MSE: 0.667\n",
      "-141\n",
      "  Epoch 710   MSE: 0.666\n",
      "-141\n",
      "  Epoch 720   MSE: 0.664\n",
      "-141\n",
      "  Epoch 730   MSE: 0.661\n",
      "-141\n",
      "  Epoch 740   MSE: 0.633\n",
      "-138\n",
      "  Epoch 750   MSE: 0.654\n",
      "-141\n",
      "  Epoch 760   MSE: 0.683\n",
      "-141\n",
      "  Epoch 770   MSE: 0.655\n",
      "-137\n",
      "  Epoch 780   MSE: 0.640\n",
      "-141\n",
      "  Epoch 790   MSE: 0.664\n",
      "-139\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(800):\n",
    "    errors = []\n",
    "    accuracy = 1\n",
    "    for input, target in zip(X_train, y_train):\n",
    "    # Forward\n",
    "        output = mlp.forward(input)\n",
    "\n",
    "    # Compute the error\n",
    "        error = target - output\n",
    "        errors.append(error)\n",
    "\n",
    "    # Back-propagate the error\n",
    "        mlp.backprop(input, error, 0.1)\n",
    "\n",
    "  # Compute the Mean Squared Error of all examples each 100 epoch\n",
    "    if epoch % 10 == 0:\n",
    "        mse = (np.array(errors) ** 2).mean()\n",
    "        print(\"  Epoch %3d   MSE: %.3f\" % (epoch, mse))\n",
    "        print(f\"{accuracy}\")\n",
    "\n",
    "        if mse <= target_mse:\n",
    "            print(\"  * Target MSE reached *\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
